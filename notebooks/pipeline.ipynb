{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run only once\n",
    "%pip install openai chromadb pandas numpy scikit-learn python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All imports OK\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€ Cell 2: Imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.data_loader  import load_episodes\n",
    "from src.get_embeddings import add_embeddings_to_df\n",
    "from src.vector_store import build_collection, get_collection\n",
    "from src.search       import semantic_search\n",
    "from src.llm_integeration import interpret_emotional_query\n",
    "from src.memory       import ConversationMemory, Turn\n",
    "from scripts.rag_pipeline import run_pipeline, print_results\n",
    "from src.url_store import add_new_urls\n",
    "\n",
    "from src.config import PODCASTS_CSV,NEW_URLS_CSV,TRANSCRIPT_EMBEDDINGS_CSV\n",
    "\n",
    "print('âœ“ All imports OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 0A: Install (once) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# %pip install youtube-transcript-api\n",
    "\n",
    "# â”€â”€ Cell 0B: Fetch all transcripts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.transcript_fetcher import fetch_all_transcripts, print_summary\n",
    "\n",
    "# Reads URLs from data/youtube_urls.txt\n",
    "# Saves results to data/processed/transcripts_df.csv\n",
    "# Logs errors to logs/transcript_errors.log\n",
    "df = fetch_all_transcripts(sleep_secs=1.0)\n",
    "\n",
    "print_summary(df)\n",
    "\n",
    "# â”€â”€ Cell 0C: Pass URLs directly (alternative) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "my_urls = [\n",
    "    \"https://www.youtube.com/watch?v=TbsRU-crgsc\",\n",
    "    \"https://www.youtube.com/watch?v=jroF3PH-PTs\",\n",
    "]\n",
    "\n",
    "df = fetch_all_transcripts(urls=my_urls)\n",
    "print_summary(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "df = pd.read_csv(TRANSCRIPT_EMBEDDINGS_CSV)\n",
    "df = df[df['status']=='success']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(TRANSCRIPT_EMBEDDINGS_CSV,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop common accidental index columns\n",
    "df = df.drop(columns=[c for c in [\"Unnamed: 0\", \"index\", \"level_0\"] if c in df.columns])\n",
    "\n",
    "# Also drop any column that starts with \"Unnamed\"\n",
    "df = df.loc[:, ~df.columns.astype(str).str.startswith(\"Unnamed\")]\n",
    "\n",
    "df.to_csv(TRANSCRIPT_EMBEDDINGS_CSV, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(TRANSCRIPT_EMBEDDINGS_CSV)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_df = add_new_urls(PODCASTS_CSV,NEW_URLS_CSV)\n",
    "urls = urls_df[\"url\"].dropna().astype(str).tolist()\n",
    "\n",
    "def processed_or_empty(path:Path)->pd.DataFrame:\n",
    "    if path.exists():\n",
    "        return pd.read_csv(path)\n",
    "    else :\n",
    "        raise FileNotFoundError(\n",
    "            f\"Transcripts file not found :{TRANSCRIPTS_CSV}\\n\"\n",
    "            \"Extract transcripts first\"\n",
    "        )\n",
    "\n",
    "#load already processed \n",
    "transcripts_df=processed_or_empty(TRANSCRIPTS_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # 3) Find truly new URLs (not already processed)\n",
    "already_processed = set(transcripts_df[\"url\"].dropna().astype(str).tolist()) if \"url\" in transcripts_df.columns else set()\n",
    "new_urls = [u for u in urls if u not in already_processed]\n",
    "\n",
    "print(f\"Total URLs in master list: {len(urls)}\")\n",
    "print(f\"New URLs to process: {len(new_urls)}\")\n",
    "\n",
    "if not new_urls:\n",
    "    print(\"No new URLs.\")\n",
    "\n",
    "# fetch transcripts only for new urls \n",
    "new_transcripts_df = fetch_all_transcripts(urls=new_urls, sleep_secs=1.0, save=False)\n",
    "\n",
    "# Append and save transcripts\n",
    "combined = pd.concat([transcripts_df, new_transcripts_df], ignore_index=True)\n",
    "#(BASE / \"data\" / \"processed\").mkdir(parents=True, exist_ok=True)\n",
    "combined.to_csv(TRANSCRIPTS_CSV, index=False)\n",
    "print(f\"Saved updated transcripts to {TRANSCRIPTS_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.read_csv(TRANSCRIPTS_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your notebook:\n",
    "from src.get_embeddings import add_embeddings_to_df\n",
    "from src.data_loader import load_episodes\n",
    "\n",
    "# Load transcripts\n",
    "df = pd.read_csv(TRANSCRIPTS_CSV)\n",
    "\n",
    "# Generate embeddings (automatically chunks long transcripts)\n",
    "df = add_embeddings_to_df(df, text_col=\"transcript_clean\")\n",
    "\n",
    "# Save\n",
    "df.to_csv(TRANSCRIPT_EMBEDDINGS_CSV, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 3: Load data with embeddings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df = load_episodes()\n",
    "df[['youtube_title', 'youtube_channel', 'duration_mins', 'word_count']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.vector_store import sync_collection\n",
    "\n",
    "collection = sync_collection(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['embedding'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 4: Build ChromaDB (RUN ONCE ONLY) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "collection = build_collection(df, force_rebuild=True)\n",
    "print(f'Collection has {collection.count()} episodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 5: Load existing collection (USE EVERY SESSION) â”€â”€â”€â”€\n",
    "collection = get_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "coerce embedding and new chroma store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded rows: 44\n",
      "Valid rows (with embeddings): 44\n",
      "Dropped rows: 0\n",
      "Embedding dim: 1536\n",
      "deleted old collection\n",
      "created collection: podcast_episodes\n",
      "âœ… Added 44 episodes to ChromaDB\n",
      "âœ“ Collection now has 44 episodes\n",
      "Chroma sync complete.\n",
      "\n",
      "âœ… Done! Collection has 44 episodes\n"
     ]
    }
   ],
   "source": [
    "# adding new episodes to chromadb\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(TRANSCRIPT_EMBEDDINGS_CSV)\n",
    "\n",
    "def coerce_embedding(x):\n",
    "    \"\"\"Return embedding as List[float] or None if invalid.\"\"\"\n",
    "    if x is None:\n",
    "        return None\n",
    "    if isinstance(x, float) and pd.isna(x):\n",
    "        return None\n",
    "\n",
    "    # already list-like\n",
    "    if isinstance(x, (list, tuple, np.ndarray)):\n",
    "        arr = np.array(x, dtype=float)\n",
    "        return arr.tolist() if arr.ndim == 1 and arr.size > 0 else None\n",
    "\n",
    "    # stored as string like \"[0.1, 0.2, ...]\"\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip()\n",
    "        if s.lower() in {\"\", \"nan\", \"none\"}:\n",
    "            return None\n",
    "        try:\n",
    "            parsed = ast.literal_eval(s)\n",
    "            arr = np.array(parsed, dtype=float)\n",
    "            return arr.tolist() if arr.ndim == 1 and arr.size > 0 else None\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    return None\n",
    "\n",
    "# ---- Clean / validate required columns ----\n",
    "required_cols = [\"video_id\", \"transcript_clean\", \"embedding\"]\n",
    "missing = [c for c in required_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"CSV is missing required columns: {missing}\")\n",
    "\n",
    "# ---- Coerce embeddings into real numeric vectors ----\n",
    "df[\"embedding\"] = df[\"embedding\"].apply(coerce_embedding)\n",
    "\n",
    "# ---- Drop invalid rows ----\n",
    "before = len(df)\n",
    "df = df.dropna(subset=[\"video_id\", \"transcript_clean\"])\n",
    "df = df[df[\"embedding\"].notna()].copy()\n",
    "after = len(df)\n",
    "\n",
    "print(f\"Loaded rows: {before}\")\n",
    "print(f\"Valid rows (with embeddings): {after}\")\n",
    "print(f\"Dropped rows: {before - after}\")\n",
    "\n",
    "# ---- Ensure consistent embedding dimension ----\n",
    "dims = df[\"embedding\"].apply(len)\n",
    "if dims.nunique() != 1:\n",
    "    common_dim = dims.value_counts().idxmax()\n",
    "    bad = (dims != common_dim).sum()\n",
    "    df = df[dims == common_dim].copy()\n",
    "    print(f\"Filtered {bad} rows with non-standard embedding dim. Using dim={common_dim}.\")\n",
    "else:\n",
    "    print(f\"Embedding dim: {dims.iloc[0]}\")\n",
    "\n",
    "# 3. Sync ChromaDB (adds only new episodes)\n",
    "from src.vector_store import build_collection\n",
    "\n",
    "# Assuming your signature is build_collection(df, add_only_new: bool)\n",
    "collection = build_collection(df, True)\n",
    "print(\"Chroma sync complete.\")\n",
    "\n",
    "print(f\"\\nâœ… Done! Collection has {collection.count()} episodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "[0.02200822673815613, 0.028621132640788954, -0.014038540743058547, 0.043925480296214424, 0.018542302047232322, 0.0137608\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Check types\n",
    "print(type(df.loc[0, \"embedding\"]))\n",
    "print(str(df.loc[0, \"embedding\"])[:120])\n",
    "print(df[\"embedding\"].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.vector_store import get_collection\n",
    "\n",
    "# Get what's currently in ChromaDB\n",
    "collection = get_collection()\n",
    "existing = collection.get()\n",
    "existing_ids = set(existing['ids'])\n",
    "\n",
    "print(f\"IDs in ChromaDB: {sorted(existing_ids)}\")\n",
    "\n",
    "# Get what's in your CSV\n",
    "df = pd.read_csv(\"data/processed/transcripts_with_embeddings.csv\")\n",
    "csv_ids = set(f\"{i+1:03d}\" for i in range(len(df)))\n",
    "\n",
    "print(f\"IDs in CSV: {sorted(csv_ids)}\")\n",
    "\n",
    "# Find overlap\n",
    "overlap = existing_ids & csv_ids\n",
    "print(f\"\\nOverlap (already in ChromaDB): {len(overlap)} episodes\")\n",
    "print(f\"New episodes: {len(csv_ids - existing_ids)}\")\n",
    "\n",
    "# These should be added:\n",
    "new_ids = sorted(csv_ids - existing_ids)\n",
    "print(f\"New IDs to add: {new_ids[:10]}...\")  # First 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.vector_store import build_collection\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(TRANSCRIPT_EMBEDDINGS_CSV)\n",
    "df = df['embedding'].dropna()\n",
    "\n",
    "# Nuclear option: delete and rebuild\n",
    "collection = build_collection(df, force_rebuild=True)\n",
    "\n",
    "# Should output:\n",
    "# ğŸ—‘ï¸  Deleted old collection\n",
    "# âœ“ Created collection: podcast_episodes\n",
    "# âš ï¸  Row 26 has no embedding â€” skipping\n",
    "# âš ï¸  Row 27 has no embedding â€” skipping\n",
    "# âœ… Added 44 episodes\n",
    "# âœ“ Collection now has 44 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(TRANSCRIPT_EMBEDDINGS_CSV)\n",
    "\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"\\nRows WITH embeddings: {df['embedding'].notna().sum()}\")\n",
    "print(f\"Rows WITHOUT embeddings: {df['embedding'].isna().sum()}\")\n",
    "\n",
    "# Check which rows are missing\n",
    "missing_idx = df[df['embedding'].isna()].index.tolist()\n",
    "print(f\"\\nMissing embedding at rows: {missing_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Before: {df['embedding'].notna().sum()}/46 have embeddings\")\n",
    "\n",
    "# Generate embeddings for rows 25 and 26\n",
    "df = add_embeddings_to_df(df)\n",
    "\n",
    "print(f\"After: {df['embedding'].notna().sum()}/46 have embeddings\")\n",
    "\n",
    "# Save\n",
    "df.to_csv(TRANSCRIPT_EMBEDDINGS_CSV, index=False)\n",
    "print(\"âœ“ Saved updated CSV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 6: Test search â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "test_queries = [\n",
    "    'I feel like a failure after not getting that job',\n",
    "    'I am angry and I do not know why',\n",
    "    'I want to love myself more',\n",
    "]\n",
    "\n",
    "for q in test_queries:\n",
    "    print(f\"\\nğŸ” '{q}'\")\n",
    "    results = semantic_search(q, collection, top_k=2)\n",
    "    for r in results:\n",
    "        print(f\"  â†’ {r['metadata']['episode_title'][:55]}  ({r['similarity']:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 7: Full RAG pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "memory     = ConversationMemory()\n",
    "user_query = 'I feel really stressed about finding a new job and keep doubting myself'\n",
    "\n",
    "output = run_pipeline(\n",
    "    user_query = user_query,\n",
    "    collection = collection,\n",
    "    memory     = memory,\n",
    "    top_k      = 3,\n",
    ")\n",
    "\n",
    "print_results(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 8: Check memory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(f'Memory has {len(memory)} turn(s)\\n')\n",
    "print(memory.build_context_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 9: Second turn (memory feeds into LLM) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "output2 = run_pipeline(\n",
    "    user_query = 'Now I feel ashamed that I am not handling this better',\n",
    "    collection = collection,\n",
    "    memory     = memory,\n",
    "    top_k      = 2,\n",
    ")\n",
    "\n",
    "print_results(output2)\n",
    "print(f'\\nMemory now has {len(memory)} turns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 10: Reset memory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "memory.clear()\n",
    "print(f'Cleared. Turns: {len(memory)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
